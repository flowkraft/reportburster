version: '3.8'

# AI Crew Development Stack
# This is a development helper service, not part of the main production apps
# Run separately from the main flowkraft services (keycloak, admin, etc.)

services:
  # ============================================
  # Frontend Applications
  # ============================================

  ai-crew-frontend:
    container_name: flowkraft-ai-crew-frontend
    build:
      context: ./ui-startpage
      dockerfile: Dockerfile
    environment:
      - NODE_ENV=production
      - LETTA_BASE_URL=http://flowkraft-ai-crew-letta:8283
      - LETTA_API_KEY=${LETTA_API_KEY:-}
      - NEXT_PUBLIC_AGENTS_CHAT_URL=${NEXT_PUBLIC_AGENTS_CHAT_URL:-http://localhost:8090}
      - ELEMENT_URL=http://flowkraft-ai-crew-element-web:80
      - CODE_SERVER_URL=http://localhost:8443
    ports:
      - "8484:3000"  # AI Crew Dashboard UI
    restart: unless-stopped
    networks:
      - ai-crew-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3000/"]
      interval: 5s
      timeout: 5s
      retries: 30
      start_period: 10s

  ai-crew-tools:
    build:
      context: ./ui-startpage
      dockerfile: Dockerfile
      target: builder
    container_name: flowkraft-ai-crew-tools
    working_dir: /app
    environment:
      - LETTA_BASE_URL=http://flowkraft-ai-crew-letta:8283
      - LETTA_API_KEY=${LETTA_API_KEY:-}
    volumes:
      - ./custom-tools:/custom-tools
      - ./agents:/agents-hq
    # No entrypoint - for running CLI commands like: docker compose run --rm ai-crew-tools npm run agents:list
    tty: true
    stdin_open: true
    networks:
      - ai-crew-network

  # ============================================
  # AI Backend Services
  # ============================================

  letta:
    build:
      context: .
      dockerfile: helpers/letta/Dockerfile
    container_name: flowkraft-ai-crew-letta
    depends_on:
      ollama:
        condition: service_healthy
    environment:
      - OPENAI_API_KEY=${OPENAI_API_KEY:-}
      - OPENAI_API_BASE=${OPENAI_API_BASE:-}
      - OLLAMA_BASE_URL=http://flowkraft-ai-crew-ollama:11434
      - AUTO_APPROVE_SHELL_COMMANDS=false
      - POSTGRES_USER=letta
      - POSTGRES_PASSWORD=${LETTA_DB_PASSWORD:-letta}
      - POSTGRES_DB=letta
    volumes:
      - letta-data:/var/lib/postgresql/data
      - ./skills:/.skills
      - ${PROJECTS_FOLDER_PATH:-./projects}:/projects
      - ${AGENTS_HQ_FOLDER_PATH:-./agents}:/agents-hq
    # Ports commented - internal services accessed via ai-crew-network
    # Uncomment only if you need direct external access for debugging
    # ports:
    #   - "8283:8283"  # Letta API
    #   - "5432:5432"  # PostgreSQL
    #   - "6379:6379"  # Redis
    restart: unless-stopped
    networks:
      - ai-crew-network
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:8283/health || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s

  ollama:
    image: ollama/ollama:0.13.5
    container_name: flowkraft-ai-crew-ollama
    volumes:
      - ollama-data:/root/.ollama
    entrypoint: ["/bin/sh", "-c"]
    command:
      - |
        # Start Ollama server in background
        /bin/ollama serve &
        OLLAMA_PID=$!

        # Wait for Ollama API to be ready (max 30 seconds)
        echo "Waiting for Ollama to start..."
        for i in $(seq 1 30); do
          if /bin/ollama list >/dev/null 2>&1; then
            echo "Ollama is ready!"
            break
          fi
          sleep 1
        done

        # Verify Ollama is actually running
        if ! /bin/ollama list >/dev/null 2>&1; then
          echo "ERROR: Ollama failed to start"
          exit 1
        fi

        # Pull embedding model (skips if already present)
        echo "Ensuring mxbai-embed-large is available..."
        if /bin/ollama pull mxbai-embed-large; then
          echo "Model ready: mxbai-embed-large"
        else
          echo "WARNING: Failed to pull model, but continuing if model exists locally"
        fi

        # Keep container running and wait for Ollama process
        echo "Ollama container ready"
        wait $OLLAMA_PID
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "/bin/ollama", "list"]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 30s
    networks:
      - ai-crew-network

  # ============================================
  # Chat & Collaboration Services
  # ============================================

  baibot:
    build:
      context: ./helpers/baibot
      dockerfile: Dockerfile
    container_name: flowkraft-ai-crew-baibot
    depends_on:
      matrix-synapse:
        condition: service_healthy
    environment:
      - BAIBOT_SEND_N_NEWEST_MESSAGES=1
      - BAIBOT_CONFIG_FILE_PATH=/app/config.yml
      - BAIBOT_PERSISTENCE_DATA_DIR_PATH=/data
    volumes:
      - ./config/baibot/config.yml:/app/config.yml:ro
      - baibot-data:/data
    restart: unless-stopped
    networks:
      - ai-crew-network

  matrix-synapse:
    image: ghcr.io/element-hq/synapse:v1.131.0
    container_name: flowkraft-ai-crew-matrix-synapse
    depends_on:
      matrix-synapsedb-postgres:
        condition: service_healthy
    environment:
      - SYNAPSE_CONFIG_PATH=/data/homeserver.yaml
      - DB_USER=${DB_USER:-synapse}
      - DB_PASSWORD=${DB_PASSWORD:-synapse}
    volumes:
      - ./config/synapse:/data
      - synapse-data:/data/media_store
    # Port commented - Matrix accessed via Element Web UI (port 8090)
    # Uncomment only if you need direct Matrix API access
    # ports:
    #   - "8008:8008"  # Matrix Synapse API
    restart: unless-stopped
    networks:
      - ai-crew-network
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:8008/health || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s

  element-web:
    image: vectorim/element-web:v1.11.102
    container_name: flowkraft-ai-crew-element-web
    volumes:
      - ./config/element/config.json:/app/config.json:ro
    ports:
      - "8090:80"  # Element Web Chat UI
    restart: unless-stopped
    networks:
      - ai-crew-network

  # ============================================
  # Database Services
  # ============================================

  matrix-synapsedb-postgres:
    image: postgres:17-alpine
    container_name: flowkraft-ai-crew-matrix-db
    volumes:
      - synapse-db-data:/var/lib/postgresql/data
    environment:
      POSTGRES_PASSWORD: ${DB_PASSWORD:-synapse}
      POSTGRES_USER: ${DB_USER:-synapse}
      POSTGRES_DB: matrix-synapsedb
      PGDATA: /var/lib/postgresql/data/pgdata
      POSTGRES_INITDB_ARGS: '--lc-collate=C --lc-ctype=C --encoding=UTF8'
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${DB_USER:-synapse} -d matrix-synapsedb"]
      interval: 10s
      timeout: 5s
      retries: 5
    restart: unless-stopped
    networks:
      - ai-crew-network

  code-server:
    image: codercom/code-server:latest
    container_name: flowkraft-ai-crew-code-server
    environment:
      - PASSWORD=${CODE_SERVER_PASSWORD:-admin}
      - SUDO_PASSWORD=${CODE_SERVER_PASSWORD:-admin}
    volumes:
      - ./agents:/home/coder/project
      - code-server-data:/home/coder/.local/share/code-server
    ports:
      - "8443:8443"  # Code Server (VS Code in browser)
    restart: unless-stopped
    networks:
      - ai-crew-network

networks:
  ai-crew-network:
    driver: bridge

volumes:
  letta-data:
  ollama-data:
  baibot-data:
  synapse-data:
  synapse-db-data:
  code-server-data:
